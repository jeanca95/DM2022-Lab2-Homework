{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Jean Carlos Guifarro\n",
    "\n",
    "Student ID: 111062422\n",
    "\n",
    "GitHub ID:jeanca95\n",
    "\n",
    "Kaggle name: Jean Guifarro\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](Place.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 25th 11:59 pm, Friday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id identification\n",
      "0        0x28cc61           test\n",
      "1        0x29e452          train\n",
      "2        0x2b3819          train\n",
      "3        0x2db41f           test\n",
      "4        0x2a2acc          train\n",
      "...           ...            ...\n",
      "1867530  0x227e25          train\n",
      "1867531  0x293813          train\n",
      "1867532  0x1e1a7e          train\n",
      "1867533  0x2156a5          train\n",
      "1867534  0x2bb9d2          train\n",
      "\n",
      "[1867535 rows x 2 columns]\n",
      "         tweet_id       emotion\n",
      "0        0x3140b1       sadness\n",
      "1        0x368b73       disgust\n",
      "2        0x296183  anticipation\n",
      "3        0x2bd6e1           joy\n",
      "4        0x2ee1dd  anticipation\n",
      "...           ...           ...\n",
      "1455558  0x38dba0           joy\n",
      "1455559  0x300ea2           joy\n",
      "1455560  0x360b99          fear\n",
      "1455561  0x22eecf           joy\n",
      "1455562  0x2fb282  anticipation\n",
      "\n",
      "[1455563 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "#import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "data_id= pd.read_csv(\"data/data_identification.csv\")\n",
    "emotion= pd.read_csv(\"data/emotion.csv\")\n",
    "\n",
    "print(data_id)\n",
    "print(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification       emotion\n",
       "0        0x29e452          train           joy\n",
       "1        0x2b3819          train           joy\n",
       "2        0x2a2acc          train         trust\n",
       "3        0x2a8830          train           joy\n",
       "4        0x20b21d          train  anticipation\n",
       "...           ...            ...           ...\n",
       "1455558  0x227e25          train       disgust\n",
       "1455559  0x293813          train       sadness\n",
       "1455560  0x1e1a7e          train           joy\n",
       "1455561  0x2156a5          train         trust\n",
       "1455562  0x2bb9d2          train         trust\n",
       "\n",
       "[1455563 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master = data_id.merge(emotion,\n",
    "                   on = [\"tweet_id\"], \n",
    "                   how = 'inner')\n",
    "df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the twitter DF\n",
    "twitter = \"data/tweets_DM.json\"\n",
    "twitter_DF = pd.read_json(twitter, lines = True)\n",
    "twitter_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeang\\AppData\\Local\\Temp\\ipykernel_16564\\2780389895.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  twitter_source.columns = twitter_source.columns.str.replace('^tweet.','')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hashtags  tweet_id  \\\n",
       "0                             [Snapchat]  0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                           [bibleverse]  0x28b412   \n",
       "3                                     []  0x1cd5b0   \n",
       "4                                     []  0x2de201   \n",
       "...                                  ...       ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                               []  0x29d0cb   \n",
       "1867532                               []  0x2a6a4f   \n",
       "1867533                               []  0x24faed   \n",
       "1867534                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Source column\n",
    "twitter_source = pd.json_normalize(twitter_DF._source, record_prefix=None)\n",
    "twitter_source.columns = twitter_source.columns.str.replace('^tweet.','')\n",
    "twitter_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hashtags  tweet_id  \\\n",
       "0                           [Snapchat]  0x376b20   \n",
       "1        [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                                   []  0x1cd5b0   \n",
       "3            [authentic, LaughOutLoud]  0x1d755c   \n",
       "4                                   []  0x2c91a8   \n",
       "...                                ...       ...   \n",
       "1455558              [NoWonder, Happy]  0x321566   \n",
       "1455559                             []  0x38959e   \n",
       "1455560                     [blessyou]  0x2cbca6   \n",
       "1455561                             []  0x24faed   \n",
       "1455562                  [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text identification  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2                      Now ISSA is stalking Tasha 😂😂😂 <LH>          train   \n",
       "3        @RISKshow @TheKevinAllison Thx for the BEST TI...          train   \n",
       "4             Still waiting on those supplies Liscus. <LH>          train   \n",
       "...                                                    ...            ...   \n",
       "1455558  I'm SO HAPPY!!! #NoWonder the name of this sho...          train   \n",
       "1455559  In every circumtance I'd like to be thankful t...          train   \n",
       "1455560  there's currently two girls walking around the...          train   \n",
       "1455561  Ah, corporate life, where you can date <LH> us...          train   \n",
       "1455562             Blessed to be living #Sundayvibes <LH>          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "2                fear  \n",
       "3                 joy  \n",
       "4        anticipation  \n",
       "...               ...  \n",
       "1455558           joy  \n",
       "1455559           joy  \n",
       "1455560           joy  \n",
       "1455561           joy  \n",
       "1455562           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train data\n",
    "train_data = twitter_source.merge(df_master)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 hashtags  tweet_id  \\\n",
       "0                            [bibleverse]  0x28b412   \n",
       "1                                      []  0x2de201   \n",
       "2       [materialism, money, possessions]  0x218443   \n",
       "3                    [GodsPlan, GodsWork]  0x2939d5   \n",
       "4                                      []  0x26289a   \n",
       "...                                   ...       ...   \n",
       "411967                                 []  0x2913b4   \n",
       "411968                                 []  0x2a980e   \n",
       "411969    [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "411970                                 []  0x29d0cb   \n",
       "411971                                 []  0x2a6a4f   \n",
       "\n",
       "                                                     text identification  \n",
       "0       Confident of your obedience, I write to you, k...           test  \n",
       "1       \"Trust is not the same as faith. A friend is s...           test  \n",
       "2       When do you have enough ? When are you satisfi...           test  \n",
       "3       God woke you up, now chase the day #GodsPlan #...           test  \n",
       "4       In these tough times, who do YOU turn to as yo...           test  \n",
       "...                                                   ...            ...  \n",
       "411967  \"For this is the message that ye heard from th...           test  \n",
       "411968  \"There is a lad here, which hath five barley l...           test  \n",
       "411969  When you buy the last 2 tickets remaining for ...           test  \n",
       "411970  I swear all this hard work gone pay off one da...           test  \n",
       "411971  @Parcel2Go no card left when I wasn't in so I ...           test  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtain test data\n",
    "test_data = twitter_source.merge(data_id[data_id['identification'] == 'test'])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56099)\t1\n",
      "  (0, 184241)\t1\n",
      "  (0, 281498)\t1\n",
      "  (0, 183684)\t1\n",
      "  (0, 278289)\t1\n",
      "  (0, 255175)\t1\n",
      "  (0, 281101)\t2\n",
      "  (0, 140117)\t1\n",
      "  (0, 247755)\t1\n",
      "  (0, 275326)\t1\n",
      "  (0, 71283)\t1\n",
      "  (0, 82756)\t1\n",
      "  (0, 168416)\t1\n",
      "  (0, 247351)\t1\n",
      "  (0, 21373)\t1\n",
      "  (0, 194247)\t1\n",
      "  (0, 2359)\t1\n",
      "  (0, 31787)\t1\n",
      "  (0, 146665)\t2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "data_counts = count_vect.fit_transform(test_data.text) #learn the vocabulary and return document-term matrix\n",
    "print(data_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neattext\n",
      "  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
      "Installing collected packages: neattext\n",
      "Successfully installed neattext-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean train text\n",
    "train_data['Clean_Text'] = train_data['text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>People post \"add #Snapchat\" dehydrated. Cuz ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>see, Trump dangerous #freepress world. &lt;LH&gt; &lt;L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>ISSA stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Thx BEST TIME tonight. stories! Heartbreakingl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>I'm HAPPY!!! #NoWonder show! #Happy! 👏👏👏👏👏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>circumtance I'd like thankful Almighty Jesus C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>there's currently girls walking library handin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Ah, corporate life, date &lt;LH&gt; relative anachro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Blessed living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hashtags  tweet_id  \\\n",
       "0                           [Snapchat]  0x376b20   \n",
       "1        [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                                   []  0x1cd5b0   \n",
       "3            [authentic, LaughOutLoud]  0x1d755c   \n",
       "4                                   []  0x2c91a8   \n",
       "...                                ...       ...   \n",
       "1455558              [NoWonder, Happy]  0x321566   \n",
       "1455559                             []  0x38959e   \n",
       "1455560                     [blessyou]  0x2cbca6   \n",
       "1455561                             []  0x24faed   \n",
       "1455562                  [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text identification  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2                      Now ISSA is stalking Tasha 😂😂😂 <LH>          train   \n",
       "3        @RISKshow @TheKevinAllison Thx for the BEST TI...          train   \n",
       "4             Still waiting on those supplies Liscus. <LH>          train   \n",
       "...                                                    ...            ...   \n",
       "1455558  I'm SO HAPPY!!! #NoWonder the name of this sho...          train   \n",
       "1455559  In every circumtance I'd like to be thankful t...          train   \n",
       "1455560  there's currently two girls walking around the...          train   \n",
       "1455561  Ah, corporate life, where you can date <LH> us...          train   \n",
       "1455562             Blessed to be living #Sundayvibes <LH>          train   \n",
       "\n",
       "              emotion                                         Clean_Text  \n",
       "0        anticipation  People post \"add #Snapchat\" dehydrated. Cuz ma...  \n",
       "1             sadness  see, Trump dangerous #freepress world. <LH> <L...  \n",
       "2                fear                       ISSA stalking Tasha 😂😂😂 <LH>  \n",
       "3                 joy  Thx BEST TIME tonight. stories! Heartbreakingl...  \n",
       "4        anticipation                      waiting supplies Liscus. <LH>  \n",
       "...               ...                                                ...  \n",
       "1455558           joy         I'm HAPPY!!! #NoWonder show! #Happy! 👏👏👏👏👏  \n",
       "1455559           joy  circumtance I'd like thankful Almighty Jesus C...  \n",
       "1455560           joy  there's currently girls walking library handin...  \n",
       "1455561           joy  Ah, corporate life, date <LH> relative anachro...  \n",
       "1455562           joy                   Blessed living #Sundayvibes <LH>  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean train text\n",
    "train_data['Clean_Text'] = train_data['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining features\n",
    "Xfeatures = train_data['Clean_Text']\n",
    "ylabels = train_data['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the logistic regression model\n",
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining train data\n",
    "x_train = Xfeatures\n",
    "y_train = ylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeang\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>Confident obedience, write you, knowing ask. (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>\"Trust faith. friend trust. Putting faith mist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>test</td>\n",
       "      <td>? satisfied ? goal money ? #materialism #money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>test</td>\n",
       "      <td>God woke up, chase day #GodsPlan #GodsWork &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>test</td>\n",
       "      <td>tough times, turn symbol hope? &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>test</td>\n",
       "      <td>\"For message ye heard beginning, love another....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>test</td>\n",
       "      <td>\"There lad here, hath barley loaves, small fis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>test</td>\n",
       "      <td>buy 2 tickets remaining sell out.. #mixedfeeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>test</td>\n",
       "      <td>swear hard work gone pay day😈💰💸 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>test</td>\n",
       "      <td>card left idea parcel &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 hashtags  tweet_id  \\\n",
       "0                            [bibleverse]  0x28b412   \n",
       "1                                      []  0x2de201   \n",
       "2       [materialism, money, possessions]  0x218443   \n",
       "3                    [GodsPlan, GodsWork]  0x2939d5   \n",
       "4                                      []  0x26289a   \n",
       "...                                   ...       ...   \n",
       "411967                                 []  0x2913b4   \n",
       "411968                                 []  0x2a980e   \n",
       "411969    [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "411970                                 []  0x29d0cb   \n",
       "411971                                 []  0x2a6a4f   \n",
       "\n",
       "                                                     text identification  \\\n",
       "0       Confident of your obedience, I write to you, k...           test   \n",
       "1       \"Trust is not the same as faith. A friend is s...           test   \n",
       "2       When do you have enough ? When are you satisfi...           test   \n",
       "3       God woke you up, now chase the day #GodsPlan #...           test   \n",
       "4       In these tough times, who do YOU turn to as yo...           test   \n",
       "...                                                   ...            ...   \n",
       "411967  \"For this is the message that ye heard from th...           test   \n",
       "411968  \"There is a lad here, which hath five barley l...           test   \n",
       "411969  When you buy the last 2 tickets remaining for ...           test   \n",
       "411970  I swear all this hard work gone pay off one da...           test   \n",
       "411971  @Parcel2Go no card left when I wasn't in so I ...           test   \n",
       "\n",
       "                                               Clean_Text  \n",
       "0       Confident obedience, write you, knowing ask. (...  \n",
       "1       \"Trust faith. friend trust. Putting faith mist...  \n",
       "2       ? satisfied ? goal money ? #materialism #money...  \n",
       "3         God woke up, chase day #GodsPlan #GodsWork <LH>  \n",
       "4                     tough times, turn symbol hope? <LH>  \n",
       "...                                                   ...  \n",
       "411967  \"For message ye heard beginning, love another....  \n",
       "411968  \"There lad here, hath barley loaves, small fis...  \n",
       "411969  buy 2 tickets remaining sell out.. #mixedfeeli...  \n",
       "411970               swear hard work gone pay day😈💰💸 <LH>  \n",
       "411971                         card left idea parcel <LH>  \n",
       "\n",
       "[411972 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning test data\n",
    "test_data['Clean_Text'] = test_data['text'].apply(nfx.remove_userhandles)\n",
    "test_data['Clean_Text'] = test_data['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['emotion'] = pipe_lr.predict(test_data.Clean_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       emotion\n",
       "0       0x28b412         trust\n",
       "1       0x2de201  anticipation\n",
       "2       0x218443           joy\n",
       "3       0x2939d5           joy\n",
       "4       0x26289a           joy\n",
       "...          ...           ...\n",
       "411967  0x2913b4  anticipation\n",
       "411968  0x2a980e  anticipation\n",
       "411969  0x316b80       sadness\n",
       "411970  0x29d0cb         anger\n",
       "411971  0x2a6a4f       sadness\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_data.drop(['hashtags','text','identification','Clean_Text'], axis=1)\n",
    "result.columns = ['id','emotion']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"Results_twitter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Report\n",
    "\n",
    "## Files\n",
    "\n",
    "When I first gave a look to the data I identify that we were given two types of documents .csv and .json. For me it was \n",
    "the first time working with .json files so this was a bit challenging for me. I first merge the \"Data identification\" and\n",
    "\"Emotion\" files. Then when I obtained the data in the .json file the data was a mess, the columns weren't divided as in the .csv files. There was many unnecesary information for building my model. I was able to create a data frame from the .json file and only obtain the information from the \"source\" column then I separated in three different columns \"hashtags\", \"tweet_id\" and \"text\". Then merged this dataframe with the previous merged one. \n",
    "\n",
    "## Data\n",
    "\n",
    "After obtaining the text I tried to create a TF-IDF table, but I had problems with the size of the table was too large for python to obtain and an error occured while running the code. So I identify that I should clean the tweets of the unnecessary words. So I research about eliminating userhandles and stopwords. The userhandles are completely unnecessary because this doesnt give us any information about emotions and the stopwords also are of no use in predicting the emotion in the tweets. So for me it was unnecessary data to have. I  was able to do this by using the neattext.functions. \n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "I  was able to clean the data using the neattext.functions. I did this data cleaning in both data, the test and train data. \n",
    "I did it by:\n",
    "##### test_data['Clean_Text'] = test_data['text'].apply(nfx.remove_userhandles)\n",
    "##### test_data['Clean_Text'] = test_data['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "With this I reduced the amount of words in the tweets significaly and I was able to start creating a model.\n",
    "\n",
    "## Model\n",
    "\n",
    "Originally as I've mentioned it previously my idea was to create a word frequency model but making the research about the data cleaning I found out a logistic regression algorithm model. So I get deep into it and decided to use this kind of model. Because this type of model has a mathematical approach to the predictions it mades. First the data is processed with PipeLine which is from the pandas library and this helps us to process and manipulate a lot of data. So this accelerates our processing and building the model time. The math behind the logistic regression algorithm is called the maximum likelihood estimation and is represented by the equation \n",
    "##### LLF = Σᵢ(𝑦ᵢ log(𝑝(𝐱ᵢ)) + (1 − 𝑦ᵢ) log(1 − 𝑝(𝐱ᵢ))).\n",
    "\n",
    "The creation of the model is: \n",
    "##### create the logistic regression model pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])\n",
    "##### pipe_lr.fit(x_train,y_train)\n",
    "\n",
    "## Results\n",
    "\n",
    "After creating the model I test it with the test data that I previously have cleaned. I obtained a 0.42402 accuracy in the kaggle competition. Hopefully this number will go up once the whole test data is uploaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
